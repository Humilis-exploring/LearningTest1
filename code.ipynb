{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3459e3-2a98-400b-adb1-332d47aea57c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:20:50.467977Z",
     "iopub.status.busy": "2024-08-02T08:20:50.467657Z",
     "iopub.status.idle": "2024-08-02T08:20:50.474881Z",
     "shell.execute_reply": "2024-08-02T08:20:50.474447Z",
     "shell.execute_reply.started": "2024-08-02T08:20:50.467959Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/install.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/install.sh\n",
    "# 切换到 conda 的环境文件夹\n",
    "cd  /opt/conda/envs \n",
    "mkdir ipex\n",
    "# 下载 ipex-llm 官方环境\n",
    "wget https://s3.idzcn.com/ipex-llm/ipex-llm-2.1.0b20240410.tar.gz \n",
    "# 解压文件夹以便恢复原先环境\n",
    "tar -zxvf ipex-llm-2.1.0b20240410.tar.gz -C ipex/ && rm ipex-llm-2.1.0b20240410.tar.gz\n",
    "# 安装 ipykernel 并将其注册到 notebook 可使用内核中\n",
    "/opt/conda/envs/ipex/bin/python3 -m pip install ipykernel && /opt/conda/envs/ipex/bin/python3 -m ipykernel install --name=ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcea61-a847-47de-94f7-523001b7f1d3",
   "metadata": {},
   "source": [
    "bash install.sh\n",
    "\n",
    "conda activate ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f2134-888d-49c7-8403-dd5830a5e700",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T08:43:49.449883Z",
     "iopub.status.busy": "2024-08-02T08:43:49.449508Z",
     "iopub.status.idle": "2024-08-02T08:44:11.524417Z",
     "shell.execute_reply": "2024-08-02T08:44:11.523950Z",
     "shell.execute_reply.started": "2024-08-02T08:43:49.449860Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:43:50,721 - modelscope - INFO - PyTorch version 2.2.2 Found.\n",
      "2024-08-02 16:43:50,722 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-08-02 16:43:50,722 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-08-02 16:43:50,774 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 8dabb305acc7e73440bd9a5a5a66fb34 and a total number of 972 components indexed\n",
      "/opt/conda/envs/ipex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 660/660 [00:00<00:00, 102kB/s]\n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 8.17kB/s]\n",
      "Downloading: 100%|██████████| 242/242 [00:00<00:00, 162kB/s]\n",
      "Downloading: 100%|██████████| 11.1k/11.1k [00:00<00:00, 6.97MB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 47.2MB/s]\n",
      "Downloading: 100%|█████████▉| 2.88G/2.88G [00:11<00:00, 278MB/s]\n",
      "Downloading: 100%|██████████| 3.47k/3.47k [00:00<00:00, 2.43MB/s]\n",
      "Downloading: 100%|██████████| 6.70M/6.70M [00:00<00:00, 48.4MB/s]\n",
      "Downloading: 100%|██████████| 1.26k/1.26k [00:00<00:00, 890kB/s]\n",
      "Downloading: 100%|██████████| 2.65M/2.65M [00:00<00:00, 33.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "# 第一个参数表示下载模型的型号，第二个参数是下载后存放的缓存地址，第三个表示版本号，默认 master\n",
    "model_dir = snapshot_download('Qwen/Qwen2-1.5B-Instruct', cache_dir='qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c4f65-a378-4b38-bbd9-b44264535430",
   "metadata": {},
   "source": [
    "pip install ipex_llm PyMuPDF llama-index-vector-stores-chroma llama-index-readers-file llama-index-embeddings-huggingface llama-index streamlit ipex_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72fd7b1a-9601-49c1-b497-f02df9451ba9",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T08:44:44.203834Z",
     "iopub.status.busy": "2024-08-02T08:44:44.203399Z",
     "iopub.status.idle": "2024-08-02T08:45:02.317278Z",
     "shell.execute_reply": "2024-08-02T08:45:02.316744Z",
     "shell.execute_reply.started": "2024-08-02T08:44:44.203813Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:44:51,650 - INFO - Converting the current model to sym_int4 format......\n",
      "2024-08-02 16:44:52,065 - INFO - PyTorch version 2.2.2 available.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    model_path = os.path.join(os.getcwd(),\"qwen2chat_src/Qwen/Qwen2-1___5B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model.save_low_bit('qwen2chat_int4')\n",
    "    tokenizer.save_pretrained('qwen2chat_int4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25a6d12-cfa3-48ee-8bb1-6dd1abc54dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:47:05.226321Z",
     "iopub.status.busy": "2024-08-02T08:47:05.225862Z",
     "iopub.status.idle": "2024-08-02T08:47:29.534875Z",
     "shell.execute_reply": "2024-08-02T08:47:29.534367Z",
     "shell.execute_reply.started": "2024-08-02T08:47:05.226300Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 130kB/s]\n",
      "Downloading: 100%|██████████| 776/776 [00:00<00:00, 532kB/s]\n",
      "Downloading: 100%|██████████| 124/124 [00:00<00:00, 74.8kB/s]\n",
      "Downloading: 100%|██████████| 47.0/47.0 [00:00<00:00, 31.5kB/s]\n",
      "Downloading: 100%|██████████| 91.4M/91.4M [00:00<00:00, 124MB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 227kB/s]\n",
      "Downloading: 100%|██████████| 91.4M/91.4M [00:00<00:00, 151MB/s]\n",
      "Downloading: 100%|██████████| 27.5k/27.5k [00:00<00:00, 5.13MB/s]\n",
      "Downloading: 100%|██████████| 52.0/52.0 [00:00<00:00, 34.5kB/s]\n",
      "Downloading: 100%|██████████| 125/125 [00:00<00:00, 81.6kB/s]\n",
      "Downloading: 100%|██████████| 429k/429k [00:00<00:00, 31.7MB/s]\n",
      "Downloading: 100%|██████████| 367/367 [00:00<00:00, 251kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 7.78MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19231ac7-ad7c-4af3-aacd-d5c2dddc2383",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T09:37:36.457012Z",
     "iopub.status.busy": "2024-08-02T09:37:36.456666Z",
     "iopub.status.idle": "2024-08-02T09:37:36.464285Z",
     "shell.execute_reply": "2024-08-02T09:37:36.463832Z",
     "shell.execute_reply.started": "2024-08-02T09:37:36.456991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/workspace/run_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_rag.py\n",
    "# 设置OpenMP线程数为8\n",
    "import os\n",
    "import time\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import torch\n",
    "from typing import Any, List, Optional, Iterator\n",
    "\n",
    "from transformers import TextStreamer, AutoTokenizer\n",
    "\n",
    "# 从llama_index库导入HuggingFaceEmbedding类，用于将文本转换为向量表示\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# 从llama_index库导入ChromaVectorStore类，用于高效存储和检索向量数据\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# 从llama_index库导入PyMuPDFReader类，用于读取和解析PDF文件内容\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "# 从llama_index库导入NodeWithScore和TextNode类\n",
    "# NodeWithScore: 表示带有相关性分数的节点，用于排序检索结果\n",
    "# TextNode: 表示文本块，是索引和检索的基本单位。节点存储文本内容及其元数据，便于构建知识图谱和语义搜索\n",
    "from llama_index.core.schema import NodeWithScore, TextNode\n",
    "# 从llama_index库导入RetrieverQueryEngine类，用于协调检索器和响应生成，执行端到端的问答过程()\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# 从llama_index库导入QueryBundle类，用于封装查询相关的信息，如查询文本、过滤器等\n",
    "from llama_index.core import QueryBundle\n",
    "# 从llama_index库导入BaseRetriever类，这是所有检索器的基类，定义了检索接口\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "# 从llama_index库导入SentenceSplitter类，用于将长文本分割成句子或语义完整的文本块，便于索引和检索\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# 从llama_index库导入VectorStoreQuery类，用于构造向量存储的查询，支持语义相似度搜索\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "# 向量数据库\n",
    "import chromadb\n",
    "from ipex_llm.llamaindex.llms import IpexLLM\n",
    "\n",
    "class Config:\n",
    "    \"\"\"配置类,存储所有需要的参数\"\"\"\n",
    "    model_path = \"qwen2chat_int4\"\n",
    "    tokenizer_path = \"qwen2chat_int4\"\n",
    "    question = \"根据道德经回答问题\"\n",
    "    data_path = \"./data/Full text of Tao Te Ching.pdf\"\n",
    "    persist_dir = \"./chroma_db\"\n",
    "    embedding_model_path = \"qwen2chat_src/AI-ModelScope/bge-small-zh-v1___5\"\n",
    "    max_new_tokens = 64\n",
    "    \n",
    "    def set_question(cls, question):\n",
    "        cls.question = question\n",
    "        \n",
    "def load_vector_database(persist_dir: str) -> ChromaVectorStore:\n",
    "    \"\"\"\n",
    "    加载或创建向量数据库\n",
    "    \n",
    "    Args:\n",
    "        persist_dir (str): 持久化目录路径\n",
    "    \n",
    "    Returns:\n",
    "        ChromaVectorStore: 向量存储对象\n",
    "    \"\"\"\n",
    "    # 检查持久化目录是否存在\n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"正在加载现有的向量数据库: {persist_dir}\")\n",
    "        chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "        chroma_collection = chroma_client.get_collection(\"llama2_paper\")\n",
    "    else:\n",
    "        print(f\"创建新的向量数据库: {persist_dir}\")\n",
    "        chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "        chroma_collection = chroma_client.create_collection(\"llama2_paper\")\n",
    "    print(f\"Vector store loaded with {chroma_collection.count()} documents\")\n",
    "    return ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "def load_data(data_path: str) -> List[TextNode]:\n",
    "    \"\"\"\n",
    "    加载并处理PDF数据\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): PDF文件路径\n",
    "    \n",
    "    Returns:\n",
    "        List[TextNode]: 处理后的文本节点列表\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFReader()\n",
    "    documents = loader.load(file_path=data_path)\n",
    "\n",
    "    text_parser = SentenceSplitter(chunk_size=384)\n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(text=text_chunk)\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"向量数据库检索器\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: ChromaVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        检索相关文档\n",
    "        \n",
    "        Args:\n",
    "            query_bundle (QueryBundle): 查询包\n",
    "        \n",
    "        Returns:\n",
    "            List[NodeWithScore]: 检索到的文档节点及其相关性得分\n",
    "        \"\"\"\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = self._vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "        print(f\"Retrieved {len(nodes_with_scores)} nodes with scores\")\n",
    "        return nodes_with_scores\n",
    "\n",
    "def completion_to_prompt(completion: str) -> str:\n",
    "    \"\"\"\n",
    "    将完成转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        completion (str): 完成的文本\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages: List[dict]) -> str:\n",
    "    \"\"\"\n",
    "    将消息列表转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        messages (List[dict]): 消息列表\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def setup_llm(config: Config) -> IpexLLM:\n",
    "    \"\"\"\n",
    "    设置语言模型\n",
    "    \n",
    "    Args:\n",
    "        config (Config): 配置对象\n",
    "    \n",
    "    Returns:\n",
    "        IpexLLM: 配置好的语言模型\n",
    "    \"\"\"\n",
    "    return IpexLLM.from_model_id_low_bit(\n",
    "        model_name=config.model_path,\n",
    "        tokenizer_name=config.tokenizer_path,\n",
    "        context_window=384,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "        model_kwargs={},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        device_map=\"cpu\",\n",
    "    )\n",
    "\n",
    "def predict()-> Iterator[str]:\n",
    "    config = Config()\n",
    "    \n",
    "    # 设置嵌入模型\n",
    "    embed_model = HuggingFaceEmbedding(model_name=config.embedding_model_path)\n",
    "    \n",
    "    # 设置语言模型\n",
    "    llm = setup_llm(config)\n",
    "    \n",
    "    # 加载向量数据库\n",
    "    vector_store = load_vector_database(persist_dir=config.persist_dir)\n",
    "    \n",
    "    # 加载和处理数据\n",
    "    nodes = load_data(data_path=config.data_path)\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "    \n",
    "    # 将 node 添加到向量存储\n",
    "    vector_store.add(nodes)\n",
    "    \n",
    "    # 设置查询\n",
    "    query_str = config.question\n",
    "    query_embedding = embed_model.get_query_embedding(query_str)\n",
    "    \n",
    "    # 执行向量存储检索\n",
    "    query_mode = \"default\"\n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    "    )\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # 处理查询结果\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "    \n",
    "    # 设置检索器\n",
    "    retriever = VectorDBRetriever(\n",
    "        vector_store, embed_model, query_mode=\"default\", similarity_top_k=1\n",
    "    )    \n",
    "    \n",
    "    # 加载对应的tokenizer ~~\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path, trust_remote_code=True)    \n",
    "    \n",
    "    # 创建文本流式输出器 ~~\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    # 创建查询引擎\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm, streaming=True)\n",
    "\n",
    "    # 执行查询\n",
    "#    start_time = time.time()\n",
    "    streaming_response = query_engine.query(query_str)\n",
    "    \n",
    "    for token in streaming_response.response_gen:\n",
    "        yield token\n",
    "     \n",
    "    \n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # 设置嵌入模型\n",
    "    embed_model = HuggingFaceEmbedding(model_name=config.embedding_model_path)\n",
    "    \n",
    "    # 设置语言模型\n",
    "    llm = setup_llm(config)\n",
    "    \n",
    "    # 加载向量数据库\n",
    "    vector_store = load_vector_database(persist_dir=config.persist_dir)\n",
    "    \n",
    "    # 加载和处理数据\n",
    "    nodes = load_data(data_path=config.data_path)\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "    \n",
    "    # 将 node 添加到向量存储\n",
    "    vector_store.add(nodes)\n",
    "    \n",
    "    # 设置查询\n",
    "    query_str = config.question\n",
    "    query_embedding = embed_model.get_query_embedding(query_str)\n",
    "    \n",
    "    # 执行向量存储检索\n",
    "    print(\"开始执行向量存储检索\")\n",
    "    query_mode = \"default\"\n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    "    )\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # 处理查询结果\n",
    "    print(\"开始处理检索结果\")\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "    \n",
    "    # 设置检索器\n",
    "    retriever = VectorDBRetriever(\n",
    "        vector_store, embed_model, query_mode=\"default\", similarity_top_k=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Query engine created with retriever: {type(retriever).__name__}\")\n",
    "    print(f\"Query string length: {len(query_str)}\")\n",
    "    print(f\"Query string: {query_str}\")\n",
    "    \n",
    "    # 创建查询引擎\n",
    "    print(\"准备与llm对话\")\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
    "\n",
    "    # 执行查询\n",
    "    print(\"开始RAG最后生成\")\n",
    "    start_time = time.time()\n",
    "    response = query_engine.query(query_str)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"------------RESPONSE GENERATION---------------------\")\n",
    "    print(str(response))\n",
    "    print(f\"inference time: {time.time()-start_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cd8ee-caf9-4112-bbcd-fce705713b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2fd26b-d322-4103-8013-946ddbb85852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T09:58:03.499256Z",
     "iopub.status.busy": "2024-08-02T09:58:03.498915Z",
     "iopub.status.idle": "2024-08-02T09:58:03.504237Z",
     "shell.execute_reply": "2024-08-02T09:58:03.503781Z",
     "shell.execute_reply.started": "2024-08-02T09:58:03.499236Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/workspace/run_streamlit_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_streamlit_stream.py\n",
    "\n",
    "\n",
    "# 导入操作系统模块，用于设置环境变量\n",
    "import os\n",
    "# 设置环境变量 OMP_NUM_THREADS 为 8，用于控制 OpenMP 线程数\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# 导入时间模块\n",
    "import time\n",
    "# 导入 Streamlit 模块，用于创建 Web 应用\n",
    "import streamlit as st\n",
    "# 从 transformers 库中导入 AutoTokenizer 类\n",
    "from transformers import AutoTokenizer\n",
    "# 从 transformers 库中导入 TextStreamer 类\n",
    "from transformers import TextStreamer, TextIteratorStreamer\n",
    "# 从 ipex_llm.transformers 库中导入 AutoModelForCausalLM 类\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "# 导入 PyTorch 库\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "import run_rag\n",
    "def generate_response(messages, message_placeholder):\n",
    "    run_rag.Config.set_question(run_rag.Config, messages[-1][\"content\"])\n",
    "    # run_rag.Config.set_question(run_rag.Config,\"天地不仁，以万物为刍狗。\")\n",
    "    return run_rag.predict()\n",
    "\n",
    "# Streamlit 应用部分\n",
    "# 设置应用标题\n",
    "st.title(\"对话道德经\")\n",
    "\n",
    "# 初始化聊天历史，如果不存在则创建一个空列表\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# 显示聊天历史\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# 用户输入部分\n",
    "if prompt := st.chat_input(\"你想说点什么?\"):\n",
    "    # 将用户消息添加到聊天历史\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    response  = str()\n",
    "    # 创建空的占位符用于显示生成的响应\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        \n",
    "        # 调用模型生成响应\n",
    "        streamer = generate_response(st.session_state.messages, message_placeholder)\n",
    "        for text in streamer:\n",
    "            response += text\n",
    "            message_placeholder.markdown(response + \"▌\")\n",
    "    \n",
    "        message_placeholder.markdown(response)\n",
    "    \n",
    "    # 将助手的响应添加到聊天历史\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a8832-dc62-4be7-abd0-73d2a653fd5c",
   "metadata": {},
   "source": [
    "streamlit run run_streamlit_stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9df5e-290e-4295-b7cd-a5f9e3700a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12baec52-1c47-40ab-b987-33c66b3ff03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
